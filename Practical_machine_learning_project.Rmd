---
title: "Course Project"
author: "Alexis Yelton"
date: "January 25, 2015"
output: html_document
---

###Loading data
```{r}
library(caret, quietly = TRUE)
setwd("~/Dropbox/PEPPERCOPY/Practical_machine_learning")

#Load data files
train <- read.csv("pml-training.csv", header = TRUE, row.names = 1, colClasses = c(rep("factor",2), rep("numeric",2), rep("factor", 2), rep("character", 153)))

test <- read.csv("pml-testing.csv", header = TRUE, row.names = 1, colClasses = c(rep("factor",2), rep("numeric",2), rep("factor", 2), rep("character", 153)))
```
###Data preprocessing
I preprocessed the data by removing the time stamps as well as all columns missing the majority of values.
```{r}
#Replace empty values with NA
train[train == ""] <- NA
test[test == ""] <- NA

#Remove columns containing NAs by looking through the data frame by column
training = train[ , ! apply(train , MARGIN = 2 , function(x) any(is.na(x)) ) ]
testing = test[, ! apply(test, MARGIN = 2, function(x) any(is.na(x)))]

#Remove timestamps, window numbers, and name of subject 
#Setting the columns to numeric values
t = as.data.frame(sapply(training[7:58], as.numeric))
final = cbind(training[59], t)
tes = as.data.frame(sapply(testing[7:58], as.numeric))
ftest = cbind(testing[59], tes)
```
###Creating training and testing data
I split the 'training' set using createDataPartition, with 0.6, splitting the training set into a (sub)training set and a custom test set.   I set the seed to 333 before splitting the training set for reproducibility.

```{r}
#Split the data into training and testing sets within the training set
set.seed(333)
inTrain <- createDataPartition(y=final$classe, p=0.6, list=FALSE)
trainFit <- final[inTrain,]
testFit <- final[-inTrain,]
```
###Examination of data
```{r}
#Examine other variables for relationship to classe
features0 <- featurePlot(x=trainFit[,c("roll_belt","pitch_belt","yaw_belt", "total_accel_belt", 
            "gyros_belt_x", "gyros_belt_y", "gyros_belt_z", "accel_belt_x",
            "accel_belt_y", "accel_belt_z", "magnet_belt_x", "magnet_belt_y")],
            y = trainFit$classe, plot="box")
#both yaw_belt and roll_belt are higher for incorrect classes
#magnet_belt_y is lower for E
features1 <- featurePlot(x=trainFit[,c("magnet_belt_z",  "roll_arm",  
            "pitch_arm", "yaw_arm", "total_accel_arm", "gyros_arm_x", 
            "gyros_arm_y",  "gyros_arm_z")], 
            y = trainFit$classe, plot="box")
#magnet_belt_z is higher for E
features2 <- featurePlot(x=trainFit[,c("accel_arm_x", "accel_arm_y", "accel_arm_z", 
            "magnet_arm_x", "magnet_arm_y", "magnet_arm_z",        
            "roll_dumbbell", "pitch_dumbbell", "yaw_dumbbell")], 
            y = trainFit$classe, plot="box")
#roll_dumbbell is higher for B and E 
features3 <- featurePlot(x=trainFit[,c("total_accel_dumbbell", "gyros_dumbbell_x", "gyros_dumbbell_y",    
                    "gyros_dumbbell_z", "accel_dumbbell_x", "accel_dumbbell_y",    
                    "accel_dumbbell_z", "magnet_dumbbell_x", "magnet_dumbbell_y"   )], 
            y = trainFit$classe, plot="box")
#magnet_dumbbell_x and magnet_dumbbell_y is higher for B
features4 <- featurePlot(x=trainFit[,c("magnet_dumbbell_z","roll_forearm","pitch_forearm",       
            "yaw_forearm","total_accel_forearm","gyros_forearm_x")], 
            y = trainFit$classe, plot="box")
#yaw_forearm higher for C
features5 <- featurePlot(x=trainFit[,c("gyros_forearm_y","gyros_forearm_z","accel_forearm_x",     
            "accel_forearm_y","accel_forearm_z","magnet_forearm_x",    
            "magnet_forearm_y","magnet_forearm_z"  )], 
            y = trainFit$classe, plot="box")
#magnet_forearm_x is lower for D and E
```
###FeaturePlot box plots example:
```{r}
plot(features2)
```

###Cross-validation and model fitting
Then I ran cross validation, 'cv' for 3 subsets of the (sub)training data. I ran train with the gbm, the boosting with trees method, on the (sub)training set. Based on the train results, the in sample accuracy was 0.96. I predicted my custom test set with the model and found out of sample accuracy (based on the confusion matrix) was estimated to be 0.96. Thus the out of sample error was estimated to be 1 – 0.96, 0.04. This model predicted all twenty of the test set classes correctly

```{r}
#Fit a model with all of the possible explanatory variables
modfit <- train(classe ~ ., data = trainFit, method = "gbm", trControl = trainControl(method = "cv", number = 3), verbose = FALSE)
testall <- predict(modfit, testFit)
confusion <- confusionMatrix(testFit$classe, predict(modfit, testFit))
#96% out of sample accuracy
confusion$overall
```
###Model predictions versus test set classes
```{r, echo=FALSE}
plot(testall ~ testFit$classe, xlab = "Test set class", ylab = "Predicted class")
```

###Alternative methods that were not used to make the final model
Note that I did examine the data first and tried two alternative methods:

1.    PCA preprocessing to deal with the high correlations of some of the predictor variables to each other. This resulted in a relatively low in sample accuracy 0.690.
2.	Removal of variables that didn’t seem to predict the classes well, based on featurePlot box plots. I picked ten variables and then ran them in a gbm model. This resulted in an in sample accuracy of 0.8742. 

```{r}
#Alternative model 1

#Look for correlations among predictor variables
library(Hmisc, quietly = TRUE)
correlations <- rcorr(as.matrix(trainFit[2:53]), type = "spearman")
correlations$r[correlations$r > 0.8 & correlations$r  < 1.0]
#A small subset of 8 variables are highly correlated with one another > 0.8 spearman correlation

#So we try PCA to combine these variables 
preProc <- preProcess(trainFit[,-1], method = "pca", pcaComp=2) #pcaComp number of PCs to compute
fitPC <- predict(preProc, trainFit[,-1])

#Predict principal components for training set
qplot(fitPC[,1],fitPC[,2], col = trainFit$classe, xlab = "PC1", ylab = "PC2")
modfitPC <- train(trainFit$classe ~ . , data = fitPC, method = "gbm", trControl = trainControl(method = "cv", number = 10), verbose = FALSE)
testPC <- predict(preProc, testFit[,-1])
PCpred <- predict(modfitPC, testPC)
confPCA <- confusionMatrix(testFit$classe, predict(modfitPC, testPC))

#Alternative model 2

#A subset of ten predictor variables appeared to be important based on featurePlot
#Fit a model with just the predictor variables identified as important through the featureplot
modfitsubset <- train(classe ~ yaw_belt + roll_belt + magnet_belt_y + magnet_belt_z + roll_dumbbell + magnet_dumbbell_x + magnet_dumbbell_y + yaw_forearm + magnet_forearm_x, data = trainFit, method = "gbm", trControl = trainControl(method = "cv", number = 3), verbose = FALSE)
confSubset <- confusionMatrix(testFit$classe, predict(modfitsubset, testFit))
```

